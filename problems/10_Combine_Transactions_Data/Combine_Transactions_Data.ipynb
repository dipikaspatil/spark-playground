{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f16d4f28-a242-4f3d-b21e-23d66b141535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-----------+----------------+------------+--------------+\n",
      "|transaction_id|region_id|customer_id|transaction_date|amount_spent|payment_method|\n",
      "+--------------+---------+-----------+----------------+------------+--------------+\n",
      "|           204|       20|       1008|      2025-06-04|       220.0|          Cash|\n",
      "|           203|       20|       1007|      2025-06-03|       180.0|    NetBanking|\n",
      "|           202|       20|       1004|      2025-06-03|       300.0|          Card|\n",
      "|           201|       20|       1003|      2025-06-02|       250.0|           UPI|\n",
      "|           104|       10|       1006|      2025-06-03|       220.0|          NULL|\n",
      "|           103|       10|       1005|      2025-06-02|       175.0|          NULL|\n",
      "|           102|       10|       1002|      2025-06-01|       200.0|          NULL|\n",
      "|           101|       10|       1001|      2025-06-01|       150.0|          NULL|\n",
      "+--------------+---------+-----------+----------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "You are given transaction records from two systems. The first dataset contains standard fields, while the second dataset includes an extra column: payment_method.\n",
    "\n",
    "Your task is to combine both datasets in a way that accommodates this schema difference. The missing column should be filled with null where not present. Sort the result by transaction_id (descending order) for readability.\n",
    "\n",
    "Input Schema & Example\n",
    "Dataset A\n",
    "Column Name\tData Type\n",
    "transaction_id\tInteger\n",
    "region_id\tInteger\n",
    "customer_id\tInteger\n",
    "transaction_date\tString\n",
    "amount_spent\tDouble\n",
    "Dataset B (with extra column payment_method)\n",
    "Column Name\tData Type\n",
    "transaction_id\tInteger\n",
    "region_id\tInteger\n",
    "customer_id\tInteger\n",
    "transaction_date\tString\n",
    "amount_spent\tDouble\n",
    "payment_method\tString\n",
    "Example Input Table (Dataset A)\n",
    "transaction_id\tregion_id\tcustomer_id\ttransaction_date\tamount_spent\n",
    "101\t10\t1001\t2025-06-01\t150.0\n",
    "Example Input Table (Dataset B)\n",
    "transaction_id\tregion_id\tcustomer_id\ttransaction_date\tamount_spent\tpayment_method\n",
    "201\t20\t1003\t2025-06-02\t250.0\tUPI\n",
    "Output Schema\n",
    "Column Name\tData Type\n",
    "transaction_id\tInteger\n",
    "region_id\tInteger\n",
    "customer_id\tInteger\n",
    "transaction_date\tString\n",
    "amount_spent\tDouble\n",
    "payment_method\tString\n",
    "Example Output Table\n",
    "transaction_id\tregion_id\tcustomer_id\ttransaction_date\tamount_spent\tpayment_method\n",
    "201\t20\t1003\t2025-06-02\t250.0\tUPI\n",
    "101\t10\t1001\t2025-06-01\t150.0\tnull\n",
    "Starter Code\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Dataset A (no payment_method)\n",
    "data_a = [\n",
    "    (101, 10, 1001, \"2025-06-01\", 150.0),\n",
    "    (102, 10, 1002, \"2025-06-01\", 200.0),\n",
    "    (103, 10, 1005, \"2025-06-02\", 175.0),\n",
    "    (104, 10, 1006, \"2025-06-03\", 220.0),\n",
    "]\n",
    "\n",
    "columns_a = [\"transaction_id\", \"region_id\", \"customer_id\", \"transaction_date\", \"amount_spent\"]\n",
    "df_a = spark.createDataFrame(data_a, columns_a)\n",
    "\n",
    "# Dataset B (with payment_method)\n",
    "data_b = [\n",
    "    (201, 20, 1003, \"2025-06-02\", 250.0, \"UPI\"),\n",
    "    (202, 20, 1004, \"2025-06-03\", 300.0, \"Card\"),\n",
    "    (203, 20, 1007, \"2025-06-03\", 180.0, \"NetBanking\"),\n",
    "    (204, 20, 1008, \"2025-06-04\", 220.0, \"Cash\"),\n",
    "]\n",
    "\n",
    "columns_b = [\"transaction_id\", \"region_id\", \"customer_id\", \"transaction_date\", \"amount_spent\", \"payment_method\"]\n",
    "df_b = spark.createDataFrame(data_b, columns_b)\n",
    "\n",
    "# Your logic goes here to create df_result\n",
    "\n",
    "display(df_result)\n",
    "'''\n",
    "\n",
    "'''\n",
    "Note - \n",
    "Note: Why Use unionByName Over union in PySpark\n",
    "\n",
    "In PySpark, you often need to combine two DataFrames vertically (row-wise). Spark provides two main methods:\n",
    "\n",
    "union()\n",
    "\n",
    "unionByName()\n",
    "\n",
    "1️⃣ union()\n",
    "\n",
    "Combines two DataFrames by column position.\n",
    "\n",
    "Requires exactly the same schema and order of columns.\n",
    "\n",
    "If one DataFrame has extra columns or a different column order, union() will fail.\n",
    "\n",
    "Example:\n",
    "\n",
    "# df_a columns: [\"id\", \"name\"]\n",
    "# df_b columns: [\"id\", \"name\", \"age\"]\n",
    "df_a.union(df_b)  # ❌ Fails due to extra column\n",
    "\n",
    "2️⃣ unionByName()\n",
    "\n",
    "Combines DataFrames by column name, not position.\n",
    "\n",
    "Handles mismatched column order.\n",
    "\n",
    "With allowMissingColumns=True, missing columns in either DataFrame are automatically filled with nulls.\n",
    "\n",
    "Example:\n",
    "\n",
    "df_result = df_a.unionByName(df_b, allowMissingColumns=True)\n",
    "\n",
    "\n",
    "df_a missing payment_method → Spark fills it with null.\n",
    "\n",
    "Columns are aligned by name, not position.\n",
    "\n",
    "Why unionByName is preferred\n",
    "Feature\t                         union()\t                            unionByName()\n",
    "Column matching\t                 By position\t                        By name\n",
    "Handles missing columns\t         ❌ No\t                                ✅ Yes (with allowMissingColumns=True)\n",
    "Flexible for schema evolution\t ❌ No\t                                ✅ Yes\n",
    "Less error-prone\t             ❌ High risk of silent misalignment     ✅ Safe alignment\n",
    "\n",
    "How Spark aligns mismatched schemas\n",
    "\n",
    "Identify all columns in both DataFrames.\n",
    "\n",
    "For missing columns in one DataFrame:\n",
    "\n",
    "Add a column with null values of the correct type.\n",
    "\n",
    "Align columns in same order.\n",
    "\n",
    "Concatenate rows vertically.\n",
    "\n",
    "Visual Example:\n",
    "\n",
    "df_a\t                        df_b\t                                  Result after unionByName\n",
    "transaction_id, amount\t        transaction_id, amount, payment_method\t  transaction_id, amount, payment_method\n",
    "101,            150\t            201,            250,    \"UPI\"\t          101,            150,    null\n",
    "...\t                            ...\t                                      201,            250,    \"UPI\"\n",
    "'''\n",
    "# Initialize Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "spark = SparkSession.builder.appName('Spark Playground').getOrCreate()\n",
    "\n",
    "# Dataset A (no payment_method)\n",
    "data_a = [\n",
    "    (101, 10, 1001, \"2025-06-01\", 150.0),\n",
    "    (102, 10, 1002, \"2025-06-01\", 200.0),\n",
    "    (103, 10, 1005, \"2025-06-02\", 175.0),\n",
    "    (104, 10, 1006, \"2025-06-03\", 220.0),\n",
    "]\n",
    "\n",
    "columns_a = [\"transaction_id\", \"region_id\", \"customer_id\", \"transaction_date\", \"amount_spent\"]\n",
    "df_a = spark.createDataFrame(data_a, columns_a)\n",
    "\n",
    "# Dataset B (with payment_method)\n",
    "data_b = [\n",
    "    (201, 20, 1003, \"2025-06-02\", 250.0, \"UPI\"),\n",
    "    (202, 20, 1004, \"2025-06-03\", 300.0, \"Card\"),\n",
    "    (203, 20, 1007, \"2025-06-03\", 180.0, \"NetBanking\"),\n",
    "    (204, 20, 1008, \"2025-06-04\", 220.0, \"Cash\"),\n",
    "]\n",
    "\n",
    "columns_b = [\"transaction_id\", \"region_id\", \"customer_id\", \"transaction_date\", \"amount_spent\", \"payment_method\"]\n",
    "df_b = spark.createDataFrame(data_b, columns_b)\n",
    "\n",
    "df_result = (\n",
    "  df_a.unionByName(df_b, allowMissingColumns=True)\n",
    "  .orderBy(F.col(\"transaction_id\").desc())\n",
    ")\n",
    "\n",
    "# Display the final DataFrame\n",
    "df_result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
