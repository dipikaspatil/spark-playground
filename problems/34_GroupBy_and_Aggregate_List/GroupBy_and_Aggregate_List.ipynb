{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dd2bc56-f6ad-478b-8ae9-484331b07970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-------------+\n",
      "|   category|sub_category|       amount|\n",
      "+-----------+------------+-------------+\n",
      "|Electronics|      Laptop|   1000, 1200|\n",
      "|  Furniture|       Chair|150, 180, 200|\n",
      "|Electronics|      iPhone|     400, 600|\n",
      "+-----------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "You are given a dataset containing transaction records with columns (category, sub_category, amount).\n",
    "\n",
    "Write a PySpark query to group the data by category and sub_category while aggregating amount into a set (no duplicates). Order the amount set in ascending order.\n",
    "\n",
    "The result should contain unique (category, sub_category) pairs with amount values in string form separated by , .\n",
    "\n",
    "Note - This question was modified on 10 Aug 2025 due to an ambiguity in the problem statement\n",
    "\n",
    "Starter Code\n",
    "Below is the starter code to create the input DataFrame:\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"sub_category\", StringType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"Electronics\", \"Laptop\", 1000),\n",
    "    (\"Electronics\", \"Laptop\", 1200),\n",
    "    (\"Furniture\", \"Chair\", 200),\n",
    "    (\"Furniture\", \"Chair\", 150),\n",
    "    (\"Furniture\", \"Chair\", 180),\n",
    "    (\"Furniture\", \"Chair\", 200),\n",
    "    (\"Electronics\",\"iPhone\", 600),\n",
    "    (\"Electronics\",\"iPhone\", 400),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "Sample Output Schema and Example Table\n",
    "Use display(df_result) to show the final DataFrame.\n",
    "\n",
    "Output Schema:\n",
    "\n",
    "Column\tType\n",
    "category\tString\n",
    "sub_category\tString\n",
    "amount\tString\n",
    "Example Output (First 2 rows):\n",
    "\n",
    "category\tsub_category\tamount\n",
    "Electronics\tLaptop\t1000, 1200\n",
    "Furniture\tChair\t150, 180, 200\n",
    "Explanation\n",
    "The dataset is grouped by category and sub_category.\n",
    "The amount values for each (category, sub_category) pair are aggregated into a sorted set.\n",
    "The final result contains unique (category, sub_category) pairs with all corresponding amount values as string separated by , .\n",
    "'''\n",
    "\n",
    "'''\n",
    "\n",
    "Bonus Challenge: Can you solve this using Spark SQL and temporary views?\n",
    "\n",
    "# Register DataFrame as temp view\n",
    "df.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT\n",
    "    category,\n",
    "    sub_category,\n",
    "    concat_ws(\n",
    "        ', ',\n",
    "        sort_array(\n",
    "            array_distinct(\n",
    "                collect_set(amount)\n",
    "            )\n",
    "        )\n",
    "    ) AS amount\n",
    "FROM transactions\n",
    "GROUP BY category, sub_category\n",
    "\"\"\"\n",
    "\n",
    "df_result = spark.sql(sql_query)\n",
    "'''\n",
    "\n",
    "# Initialize Spark session\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName('Spark Playground').getOrCreate()\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"sub_category\", StringType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"Electronics\", \"Laptop\", 1000),\n",
    "    (\"Electronics\", \"Laptop\", 1200),\n",
    "    (\"Furniture\", \"Chair\", 200),\n",
    "    (\"Furniture\", \"Chair\", 150),\n",
    "    (\"Furniture\", \"Chair\", 180),\n",
    "    (\"Furniture\", \"Chair\", 200),\n",
    "    (\"Electronics\",\"iPhone\", 600),\n",
    "    (\"Electronics\",\"iPhone\", 400),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df_result = (\n",
    "  df.groupBy(\"category\", \"sub_category\")\n",
    "  .agg(\n",
    "    F.array_sort( # sort ascending\n",
    "      F.array_distinct( # remove duplicates\n",
    "        F.collect_list(F.col(\"amount\")) # gather all values\n",
    "      )\n",
    "    ).alias(\"amount_list\")\n",
    "  )\n",
    "  .withColumn(\"amount\", F.concat_ws(\", \", F.col(\"amount_list\"))) # concat with seperator\n",
    "  .select(\"category\", \"sub_category\", \"amount\")\n",
    ")\n",
    "\n",
    "\n",
    "# Display result\n",
    "df_result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
