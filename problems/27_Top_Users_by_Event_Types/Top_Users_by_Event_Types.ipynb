{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a44c5e0-9d0f-4a2b-adb4-0a63ae6aeeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+------------+----+\n",
      "|      date|user_id|distinct_event_types|total_events|rank|\n",
      "+----------+-------+--------------------+------------+----+\n",
      "|2025-06-01|     u1|                   2|           3|   1|\n",
      "|2025-06-01|     u2|                   2|           2|   2|\n",
      "|2025-06-01|     u3|                   1|           2|   3|\n",
      "|2025-06-02|     u2|                   3|           3|   1|\n",
      "|2025-06-02|     u1|                   2|           2|   2|\n",
      "|2025-06-02|     u3|                   1|           1|   3|\n",
      "|2025-06-03|     u6|                   3|           4|   1|\n",
      "|2025-06-03|     u5|                   3|           3|   2|\n",
      "|2025-06-03|     u7|                   1|           1|   3|\n",
      "|2025-06-04|     u1|                   1|           1|   1|\n",
      "|2025-06-04|     u2|                   1|           1|   2|\n",
      "+----------+-------+--------------------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "You are given an events dataset where each record contains a user_id, timestamp, and event_type.\n",
    "\n",
    "Your task is to compute, for each calendar day, the top 3 users with the highest number of distinct event types that occurred on that day. Break ties by:\n",
    "\n",
    "Higher total events that day (count of rows for that user-day).\n",
    "Then lexicographically smaller user_id.\n",
    "Return the result sorted by date (ascending), rank (ascending), and user_id (ascending) for readability.\n",
    "\n",
    "Input Schema & Example\n",
    "Column Name\tData Type\n",
    "user_id\tString\n",
    "timestamp\tString\n",
    "event_type\tString\n",
    "Output Schema\n",
    "Column Name\tData Type\n",
    "date\tDate\n",
    "user_id\tString\n",
    "distinct_event_types\tInteger\n",
    "total_events\tInteger\n",
    "rank\tInteger\n",
    "rank ranges from 1 to 3 per date, representing the userâ€™s position for that day.\n",
    "Starter Code\n",
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data = [\n",
    "    # 2025-06-01\n",
    "    (\"u1\", \"2025-06-01 00:10:00\", \"click\"),\n",
    "    (\"u1\", \"2025-06-01 01:20:00\", \"view\"),\n",
    "    (\"u1\", \"2025-06-01 02:30:00\", \"click\"),\n",
    "    (\"u2\", \"2025-06-01 10:00:00\", \"view\"),\n",
    "    (\"u2\", \"2025-06-01 11:00:00\", \"purchase\"),\n",
    "    (\"u3\", \"2025-06-01 12:00:00\", \"view\"),\n",
    "    (\"u3\", \"2025-06-01 13:00:00\", \"view\"),\n",
    "    (\"u4\", \"2025-06-01 23:59:59\", \"click\"),\n",
    "\n",
    "    # 2025-06-02\n",
    "    (\"u1\", \"2025-06-02 00:00:01\", \"view\"),\n",
    "    (\"u1\", \"2025-06-02 08:00:00\", \"purchase\"),\n",
    "    (\"u2\", \"2025-06-02 09:00:00\", \"click\"),\n",
    "    (\"u2\", \"2025-06-02 09:05:00\", \"view\"),\n",
    "    (\"u2\", \"2025-06-02 09:10:00\", \"share\"),\n",
    "    (\"u3\", \"2025-06-02 22:00:00\", \"view\"),\n",
    "\n",
    "    (\"u5\", \"2025-06-03 10:00:00\", \"click\"),\n",
    "    (\"u5\", \"2025-06-03 10:01:00\", \"view\"),\n",
    "    (\"u5\", \"2025-06-03 10:02:00\", \"share\"),\n",
    "    (\"u6\", \"2025-06-03 10:03:00\", \"click\"),\n",
    "    (\"u6\", \"2025-06-03 10:04:00\", \"view\"),\n",
    "    (\"u6\", \"2025-06-03 10:05:00\", \"share\"),\n",
    "    (\"u6\", \"2025-06-03 10:06:00\", \"view\"),\n",
    "    (\"u7\", \"2025-06-03 11:00:00\", \"view\"),\n",
    "\n",
    "    (\"u1\", \"2025-06-04 09:00:00\", \"click\"),\n",
    "    (\"u2\", \"2025-06-04 09:05:00\", \"click\"),\n",
    "\n",
    "    (\"u8\", \"2025-06-01 23:59:59\", \"view\"),\n",
    "    (\"u8\", \"2025-06-02 00:00:00\", \"view\"),\n",
    "]\n",
    "\n",
    "columns = [\"user_id\", \"timestamp\", \"event_type\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Your logic goes here to create df_result\n",
    "\n",
    "display(df_result)\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "Bonus Challenge: Can you solve this using Spark SQL and temporary views?\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 1: Create a temporary view\n",
    "# ----------------------------------------\n",
    "df.createOrReplaceTempView(\"events\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 2: Aggregate per user per day\n",
    "# ----------------------------------------\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW user_day_agg AS\n",
    "    SELECT\n",
    "        user_id,\n",
    "        DATE(timestamp) AS date,\n",
    "        COUNT(DISTINCT event_type) AS distinct_event_types,\n",
    "        COUNT(*) AS total_events\n",
    "    FROM events\n",
    "    GROUP BY user_id, DATE(timestamp)\n",
    "\"\"\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 3: Rank users per day using window function\n",
    "# ----------------------------------------\n",
    "df_result = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM (\n",
    "        SELECT\n",
    "            user_id,\n",
    "            DATE(timestamp) AS date,\n",
    "            COUNT(DISTINCT event_type) AS distinct_event_types,\n",
    "            COUNT(*) AS total_events,\n",
    "            ROW_NUMBER() OVER (\n",
    "                PARTITION BY DATE(timestamp)\n",
    "                ORDER BY COUNT(DISTINCT event_type) DESC, COUNT(*) DESC, user_id ASC\n",
    "            ) AS rank\n",
    "        FROM events\n",
    "        GROUP BY user_id, DATE(timestamp)\n",
    "    ) ranked\n",
    "    WHERE rank <= 3\n",
    "    ORDER BY date ASC, rank ASC, user_id ASC\n",
    "\"\"\")\n",
    "\n",
    "# Reorder columns to match expected schema\n",
    "df_result = df_result.select(\"date\", \"user_id\", \"distinct_event_types\", \"total_events\", \"rank\")\n",
    "\n",
    "'''\n",
    "\n",
    "# Initialize Spark session\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName('Spark Playground').getOrCreate()\n",
    "\n",
    "data = [\n",
    "    # 2025-06-01\n",
    "    (\"u1\", \"2025-06-01 00:10:00\", \"click\"),\n",
    "    (\"u1\", \"2025-06-01 01:20:00\", \"view\"),\n",
    "    (\"u1\", \"2025-06-01 02:30:00\", \"click\"),\n",
    "    (\"u2\", \"2025-06-01 10:00:00\", \"view\"),\n",
    "    (\"u2\", \"2025-06-01 11:00:00\", \"purchase\"),\n",
    "    (\"u3\", \"2025-06-01 12:00:00\", \"view\"),\n",
    "    (\"u3\", \"2025-06-01 13:00:00\", \"view\"),\n",
    "    (\"u4\", \"2025-06-01 23:59:59\", \"click\"),\n",
    "\n",
    "    # 2025-06-02\n",
    "    (\"u1\", \"2025-06-02 00:00:01\", \"view\"),\n",
    "    (\"u1\", \"2025-06-02 08:00:00\", \"purchase\"),\n",
    "    (\"u2\", \"2025-06-02 09:00:00\", \"click\"),\n",
    "    (\"u2\", \"2025-06-02 09:05:00\", \"view\"),\n",
    "    (\"u2\", \"2025-06-02 09:10:00\", \"share\"),\n",
    "    (\"u3\", \"2025-06-02 22:00:00\", \"view\"),\n",
    "\n",
    "    (\"u5\", \"2025-06-03 10:00:00\", \"click\"),\n",
    "    (\"u5\", \"2025-06-03 10:01:00\", \"view\"),\n",
    "    (\"u5\", \"2025-06-03 10:02:00\", \"share\"),\n",
    "    (\"u6\", \"2025-06-03 10:03:00\", \"click\"),\n",
    "    (\"u6\", \"2025-06-03 10:04:00\", \"view\"),\n",
    "    (\"u6\", \"2025-06-03 10:05:00\", \"share\"),\n",
    "    (\"u6\", \"2025-06-03 10:06:00\", \"view\"),\n",
    "    (\"u7\", \"2025-06-03 11:00:00\", \"view\"),\n",
    "\n",
    "    (\"u1\", \"2025-06-04 09:00:00\", \"click\"),\n",
    "    (\"u2\", \"2025-06-04 09:05:00\", \"click\"),\n",
    "\n",
    "    (\"u8\", \"2025-06-01 23:59:59\", \"view\"),\n",
    "    (\"u8\", \"2025-06-02 00:00:00\", \"view\"),\n",
    "]\n",
    "\n",
    "columns = [\"user_id\", \"timestamp\", \"event_type\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Extract date from timestamp and calculate aggregates per user per day\n",
    "df_with_extra_columns = (\n",
    "  df.withColumn(\"date\", F.to_date(F.to_timestamp(\"timestamp\", \"yyyy-MM-dd HH:mm:ss\"), \"yyyy-MM-dd\"))\n",
    "  .groupBy(F.col(\"date\"), F.col(\"user_id\"))\n",
    "  .agg(\n",
    "    F.countDistinct(F.col(\"event_type\")).alias(\"distinct_event_types\"), # Count of unique event types\n",
    "    F.count(F.col(\"event_type\")).alias(\"total_events\") # Total events per user per day\n",
    "  )\n",
    ")\n",
    "\n",
    "# Define window specification for ranking\n",
    "window_spec = Window.partitionBy(\"date\").orderBy(\n",
    "  F.col(\"distinct_event_types\").desc(), # Higher unique event types first\n",
    "  F.col(\"total_events\").desc(), # Break ties with total events\n",
    "  \"user_id\" # Break further ties lexicographically\n",
    ")\n",
    "\n",
    "# Apply row_number to rank users per day and filter top 3\n",
    "df_result = (\n",
    "  df_with_extra_columns.withColumn(\"rank\", F.row_number().over(window_spec)) # Assign rank\n",
    "  .filter(F.col(\"rank\") <= 3) # Keep only top 3 users per day\n",
    "  .orderBy(\"date\", \"rank\", \"user_id\") # Sort final output for readability\n",
    ")\n",
    "\n",
    "# Display results\n",
    "df_result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
