{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fa583b9-1987-438e-9325-40211e7bcd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      word|count|\n",
      "+----------+-----+\n",
      "|     hello|    4|\n",
      "|   PySpark|    3|\n",
      "|        is|    2|\n",
      "|     world|    2|\n",
      "|     Spark|    2|\n",
      "|       fun|    1|\n",
      "|     makes|    1|\n",
      "|     again|    1|\n",
      "|      data|    1|\n",
      "|       big|    1|\n",
      "|processing|    1|\n",
      "|      easy|    1|\n",
      "|  powerful|    1|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Write a PySpark program to count the occurrences of each word in a given text file. The solution must utilize RDD transformations and actions for processing, and then convert the final RDD into a DataFrame for output. Sort DataFrame by count in descending order.\n",
    "\n",
    "Hint\n",
    "To convert an RDD to a DataFrame, use the spark.createDataFrame() method. Example:\n",
    "\n",
    "df = spark.createDataFrame(rdd, schema=[\"word\", \"count\"])\n",
    "Input\n",
    "The input data will be provided as a plain text file located at /datasets/notes.txt. Each line contains a sentence or phrase. Example input text:\n",
    "\n",
    "hello world\n",
    "hello PySpark\n",
    "PySpark is fun\n",
    "hello world again\n",
    "Output\n",
    "Sample Output Schema\n",
    "word: String\n",
    "count: Integer\n",
    "Example Table\n",
    "word\tcount\n",
    "hello\t3\n",
    "world\t2\n",
    "PySpark\t2\n",
    "is\t1\n",
    "fun\t1\n",
    "again\t1\n",
    "Explanation\n",
    "The program reads the text file into an RDD.\n",
    "The RDD is processed to split lines into words, count the occurrences of each word, and sort by word.\n",
    "The final RDD is converted into a DataFrame with columns word and count and sorted by count in descending order.\n",
    "Use display(df) to show the final DataFrame.\n",
    "'''\n",
    "\n",
    "'''\n",
    "üö® When to Use RDD\n",
    "\n",
    "Use RDD only if:\n",
    "\n",
    "Data is completely unstructured\n",
    "Need low-level control\n",
    "Custom partitioning logic\n",
    "Legacy Spark code\n",
    "\n",
    "‚úÖ When to Use DataFrame (Almost Always)\n",
    "\n",
    "Use DataFrame when:\n",
    "‚úî Working with structured/semi-structured data\n",
    "‚úî Need performance\n",
    "‚úî Writing SQL queries\n",
    "‚úî Using Spark 2.x+\n",
    "\n",
    "95% of Spark jobs should use DataFrames\n",
    "\n",
    "what kind of fine grain control rdd provides which is missing in spark?\n",
    "\n",
    "When people say ‚ÄúRDD gives fine-grained control‚Äù, they mean control over execution behavior and \n",
    "data handling that DataFrames intentionally hide for optimization and simplicity.\n",
    "\n",
    "üîç What ‚ÄúFine-Grained Control‚Äù RDD Provides (That DataFrames Don‚Äôt)\n",
    "1Ô∏è‚É£ Custom Partitioning Logic\n",
    "RDD\n",
    "\n",
    "You can define exactly how keys are distributed across partitions.\n",
    "\n",
    "rdd.partitionBy(\n",
    "    numPartitions=10,\n",
    "    partitionFunc=lambda key: hash(key) % 10\n",
    ")\n",
    "\n",
    "\n",
    "‚úî Control data locality\n",
    "‚úî Reduce shuffle for joins\n",
    "‚úî Optimize skewed keys\n",
    "\n",
    "DataFrame\n",
    "\n",
    "‚ùå No custom partition function\n",
    "‚úî Only repartition() or partitionBy() (coarse-grained)\n",
    "\n",
    "2Ô∏è‚É£ Control Over Data Serialization\n",
    "RDD\n",
    "\n",
    "You control:\n",
    "\n",
    "Serialization format\n",
    "\n",
    "Object representation\n",
    "\n",
    "Custom serializers (Kryo tuning)\n",
    "\n",
    "spark.conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "\n",
    "\n",
    "RDD lets you tune object-level serialization.\n",
    "\n",
    "DataFrame\n",
    "\n",
    "‚ùå Serialization is managed internally\n",
    "‚úî Columnar + Tungsten (fast but opaque)\n",
    "\n",
    "3Ô∏è‚É£ Processing Unstructured / Irregular Data\n",
    "RDD\n",
    "\n",
    "Can process:\n",
    "\n",
    "Free-text\n",
    "\n",
    "Logs\n",
    "\n",
    "Binary blobs\n",
    "\n",
    "Irregular nested formats\n",
    "\n",
    "rdd.flatMap(lambda line: custom_parse(line))\n",
    "\n",
    "DataFrame\n",
    "\n",
    "‚ùå Needs schema\n",
    "‚úî Best for structured / semi-structured data\n",
    "\n",
    "4Ô∏è‚É£ Custom Stateful Computation\n",
    "RDD\n",
    "\n",
    "You can maintain state inside transformations.\n",
    "\n",
    "rdd.mapPartitions(lambda it: custom_stateful_logic(it))\n",
    "\n",
    "\n",
    "‚úî Full control per partition\n",
    "‚úî Stateful streaming logic (legacy)\n",
    "\n",
    "DataFrame\n",
    "\n",
    "‚ùå Stateless by design\n",
    "‚úî Only declarative aggregations\n",
    "\n",
    "5Ô∏è‚É£ Control Over Memory Usage\n",
    "RDD\n",
    "\n",
    "You can choose:\n",
    "\n",
    "Storage level\n",
    "\n",
    "Serialization strategy\n",
    "\n",
    "rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "\n",
    "\n",
    "‚úî Fine memory tuning\n",
    "\n",
    "DataFrame\n",
    "\n",
    "‚úî Can cache\n",
    "‚ùå Cannot control serialization details\n",
    "\n",
    "6Ô∏è‚É£ Side Effects & External Systems\n",
    "RDD\n",
    "\n",
    "Allows:\n",
    "\n",
    "Writing to external systems\n",
    "\n",
    "Network calls\n",
    "\n",
    "Custom I/O\n",
    "\n",
    "rdd.foreachPartition(send_to_api)\n",
    "\n",
    "DataFrame\n",
    "\n",
    "‚ùå Discouraged\n",
    "‚úî Must use structured sinks\n",
    "\n",
    "7Ô∏è‚É£ Deterministic Execution Order (Within Partition)\n",
    "RDD\n",
    "\n",
    "Order is preserved inside partitions.\n",
    "\n",
    "rdd.mapPartitions(process_in_order)\n",
    "\n",
    "DataFrame\n",
    "\n",
    "‚ùå Order is NOT guaranteed unless explicitly sorted\n",
    "\n",
    "8Ô∏è‚É£ Custom Error Handling & Retry Logic\n",
    "RDD\n",
    "\n",
    "You can implement:\n",
    "\n",
    "Try/catch per record\n",
    "\n",
    "Fallback logic\n",
    "\n",
    "rdd.map(lambda x: safe_parse(x))\n",
    "\n",
    "DataFrame\n",
    "\n",
    "‚ùå Errors fail the whole job\n",
    "‚úî Limited via try_cast (SQL)\n",
    "\n",
    "‚ö† Why Spark Hides This in DataFrames\n",
    "\n",
    "Spark intentionally removes fine-grained control in DataFrames to:\n",
    "\n",
    "‚úî Enable Catalyst optimization\n",
    "‚úî Enable predicate pushdown\n",
    "‚úî Enable vectorized execution\n",
    "‚úî Reduce developer errors\n",
    "‚úî Improve performance\n",
    "\n",
    "You trade control for speed and safety.\n",
    "\n",
    "üéØ Interview-Ready Summary\n",
    "\n",
    "RDDs provide fine-grained control over data partitioning, serialization, stateful computation, memory management, and custom execution logic.\n",
    "DataFrames abstract these details to enable powerful optimizations through Catalyst and Tungsten.\n",
    "RDDs are useful for unstructured data or custom processing, while DataFrames are preferred for most analytics workloads.\n",
    "\n",
    "'''\n",
    "\n",
    "# Initialize Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName('Spark Playground').getOrCreate()\n",
    "\n",
    "rdd = spark.sparkContext.textFile(\"./sample_dataset.txt\")\n",
    "\n",
    "word_count_rdd = (\n",
    "  rdd.flatMap(lambda x : x.split(\" \")) # split lines into words\n",
    "      .map(lambda word : word.strip())             # remove extra spaces\n",
    "      .filter(lambda word : word != \"\")            # remove empty words\n",
    "      .map(lambda word : (word, 1)) # (word, 1)\n",
    "      .reduceByKey(lambda a, b : a + b) # count words\n",
    ")\n",
    "\n",
    "'''\n",
    "‚úî flatMap ‚Üí converts lines into individual words\n",
    "‚úî map ‚Üí creates (word, 1) pairs\n",
    "‚úî reduceByKey ‚Üí efficiently aggregates counts\n",
    "'''\n",
    "\n",
    "df = spark.createDataFrame(word_count_rdd, schema=[\"word\", \"count\"])\n",
    "\n",
    "df_result = df.sort(F.col(\"count\").desc())\n",
    "\n",
    "# Display the final DataFrame using the display() function.\n",
    "df_result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
