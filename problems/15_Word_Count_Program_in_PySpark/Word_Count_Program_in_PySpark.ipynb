{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fa583b9-1987-438e-9325-40211e7bcd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      word|count|\n",
      "+----------+-----+\n",
      "|     hello|    4|\n",
      "|   PySpark|    3|\n",
      "|        is|    2|\n",
      "|     world|    2|\n",
      "|     Spark|    2|\n",
      "|       fun|    1|\n",
      "|     makes|    1|\n",
      "|     again|    1|\n",
      "|      data|    1|\n",
      "|       big|    1|\n",
      "|processing|    1|\n",
      "|      easy|    1|\n",
      "|  powerful|    1|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Write a PySpark program to count the occurrences of each word in a given text file. The solution must utilize RDD transformations and actions for processing, and then convert the final RDD into a DataFrame for output. Sort DataFrame by count in descending order.\n",
    "\n",
    "Hint\n",
    "To convert an RDD to a DataFrame, use the spark.createDataFrame() method. Example:\n",
    "\n",
    "df = spark.createDataFrame(rdd, schema=[\"word\", \"count\"])\n",
    "Input\n",
    "The input data will be provided as a plain text file located at /datasets/notes.txt. Each line contains a sentence or phrase. Example input text:\n",
    "\n",
    "hello world\n",
    "hello PySpark\n",
    "PySpark is fun\n",
    "hello world again\n",
    "Output\n",
    "Sample Output Schema\n",
    "word: String\n",
    "count: Integer\n",
    "Example Table\n",
    "word\tcount\n",
    "hello\t3\n",
    "world\t2\n",
    "PySpark\t2\n",
    "is\t1\n",
    "fun\t1\n",
    "again\t1\n",
    "Explanation\n",
    "The program reads the text file into an RDD.\n",
    "The RDD is processed to split lines into words, count the occurrences of each word, and sort by word.\n",
    "The final RDD is converted into a DataFrame with columns word and count and sorted by count in descending order.\n",
    "Use display(df) to show the final DataFrame.\n",
    "'''\n",
    "\n",
    "'''\n",
    "üö® When to Use RDD\n",
    "\n",
    "Use RDD only if:\n",
    "\n",
    "Data is completely unstructured\n",
    "Need low-level control\n",
    "Custom partitioning logic\n",
    "Legacy Spark code\n",
    "\n",
    "‚úÖ When to Use DataFrame (Almost Always)\n",
    "\n",
    "Use DataFrame when:\n",
    "‚úî Working with structured/semi-structured data\n",
    "‚úî Need performance\n",
    "‚úî Writing SQL queries\n",
    "‚úî Using Spark 2.x+\n",
    "\n",
    "95% of Spark jobs should use DataFrames\n",
    "\n",
    "what kind of fine grain control rdd provides which is missing in spark?\n",
    "\n",
    "When people say ‚ÄúRDD gives fine-grained control‚Äù, they mean control over execution behavior and \n",
    "data handling that DataFrames intentionally hide for optimization and simplicity.\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Initialize Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName('Spark Playground').getOrCreate()\n",
    "\n",
    "rdd = spark.sparkContext.textFile(\"./sample_dataset.txt\")\n",
    "\n",
    "word_count_rdd = (\n",
    "  rdd.flatMap(lambda x : x.split(\" \")) # split lines into words\n",
    "      .map(lambda word : word.strip())             # remove extra spaces\n",
    "      .filter(lambda word : word != \"\")            # remove empty words\n",
    "      .map(lambda word : (word, 1)) # (word, 1)\n",
    "      .reduceByKey(lambda a, b : a + b) # count words\n",
    ")\n",
    "\n",
    "'''\n",
    "‚úî flatMap ‚Üí converts lines into individual words\n",
    "‚úî map ‚Üí creates (word, 1) pairs\n",
    "‚úî reduceByKey ‚Üí efficiently aggregates counts\n",
    "'''\n",
    "\n",
    "df = spark.createDataFrame(word_count_rdd, schema=[\"word\", \"count\"])\n",
    "\n",
    "df_result = df.sort(F.col(\"count\").desc())\n",
    "\n",
    "# Display the final DataFrame using the display() function.\n",
    "df_result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
