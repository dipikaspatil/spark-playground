{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdaacdda-b2db-4cef-aa2d-8cf861c9f740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------+-----+\n",
      "|employee_id|Present|Absent|Leave|\n",
      "+-----------+-------+------+-----+\n",
      "|          1|      2|     1|    0|\n",
      "|          3|      1|     2|    0|\n",
      "|          2|      1|     0|    1|\n",
      "|          4|      1|     0|    2|\n",
      "+-----------+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "You are given a dataset containing employee attendance records across various departments in a company. Each row contains:\n",
    "\n",
    "Employee ID\n",
    "Department Name\n",
    "Date\n",
    "Status (Present / Absent / Leave)\n",
    "Your task is to pivot the data such that for each employee_id, you display the count of each attendance status (Present, Absent, Leave) as separate columns.\n",
    "\n",
    "Input Schema & Example\n",
    "Column Name\tData Type\n",
    "employee_id\tInteger\n",
    "department\tString\n",
    "date\tString\n",
    "status\tString\n",
    "Example Input Table\n",
    "employee_id\tdepartment\tdate\tstatus\n",
    "1\tHR\t2025-07-01\tPresent\n",
    "1\tHR\t2025-07-02\tAbsent\n",
    "2\tFinance\t2025-07-01\tLeave\n",
    "Output Schema\n",
    "Column Name\tData Type\n",
    "employee_id\tInteger\n",
    "Present\tInteger\n",
    "Absent\tInteger\n",
    "Leave\tInteger\n",
    "Example Output Table\n",
    "employee_id\tPresent\tAbsent\tLeave\n",
    "1\t1\t1\t0\n",
    "2\t0\t0\t1\n",
    "ðŸ’¡ Explanation\n",
    "Employee 1 has 2 records: 1 Present and 1 Absent.\n",
    "Employee 2 has 1 Leave record.\n",
    "The output shows total counts of each status (as columns) per employee.\n",
    "Starter Code\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (1, \"HR\", \"2025-07-01\", \"Present\"),\n",
    "    (1, \"HR\", \"2025-07-02\", \"Absent\"),\n",
    "    (1, \"HR\", \"2025-07-03\", \"Present\"),\n",
    "    (2, \"Finance\", \"2025-07-01\", \"Leave\"),\n",
    "    (2, \"Finance\", \"2025-07-02\", \"Present\"),\n",
    "    (3, \"IT\", \"2025-07-01\", \"Absent\"),\n",
    "    (3, \"IT\", \"2025-07-02\", \"Absent\"),\n",
    "    (3, \"IT\", \"2025-07-03\", \"Present\"),\n",
    "    (4, \"IT\", \"2025-07-01\", \"Leave\"),\n",
    "    (4, \"IT\", \"2025-07-02\", \"Leave\"),\n",
    "    (4, \"IT\", \"2025-07-03\", \"Present\"),\n",
    "]\n",
    "\n",
    "columns = [\"employee_id\", \"department\", \"date\", \"status\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Your logic goes here to create df_result\n",
    "\n",
    "display(df_result)\n",
    "'''\n",
    "\n",
    "# Initialize Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName('Spark Playground').getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (1, \"HR\", \"2025-07-01\", \"Present\"),\n",
    "    (1, \"HR\", \"2025-07-02\", \"Absent\"),\n",
    "    (1, \"HR\", \"2025-07-03\", \"Present\"),\n",
    "    (2, \"Finance\", \"2025-07-01\", \"Leave\"),\n",
    "    (2, \"Finance\", \"2025-07-02\", \"Present\"),\n",
    "    (3, \"IT\", \"2025-07-01\", \"Absent\"),\n",
    "    (3, \"IT\", \"2025-07-02\", \"Absent\"),\n",
    "    (3, \"IT\", \"2025-07-03\", \"Present\"),\n",
    "    (4, \"IT\", \"2025-07-01\", \"Leave\"),\n",
    "    (4, \"IT\", \"2025-07-02\", \"Leave\"),\n",
    "    (4, \"IT\", \"2025-07-03\", \"Present\"),\n",
    "]\n",
    "\n",
    "columns = [\"employee_id\", \"department\", \"date\", \"status\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Using pivot\n",
    "df_result = (\n",
    "  df.groupBy(\"employee_id\")\n",
    "  .pivot(\"status\", [\"Present\", \"Absent\", \"Leave\"])\n",
    "  .count()\n",
    "  .fillna(0)\n",
    ")\n",
    "\n",
    "'''\n",
    "Alternate solution - \n",
    "df_result = (\n",
    "  df.groupBy(\"employee_id\")\n",
    "  .agg(\n",
    "    F.sum(F.when(F.col(\"status\") == \"Present\", 1).otherwise(0)).alias(\"Present\"),\n",
    "    F.sum(F.when(F.col(\"status\") == \"Absent\", 1).otherwise(0)).alias(\"Absent\"),\n",
    "    F.sum(F.when(F.col(\"status\") == \"Leave\", 1).otherwise(0)).alias(\"Leave\")\n",
    "  )\n",
    ")\n",
    "'''\n",
    "\n",
    "'''\n",
    "Bonus Challenge: Can you solve this using Spark SQL and temporary views?\n",
    "\n",
    "df.createOrReplaceTempView(\"attendance\")\n",
    "\n",
    "df_result = spark.sql(\"\"\"\n",
    "  SELECT\n",
    "    employee_id,\n",
    "    SUM(CASE WHEN status = 'Present' THEN 1 ELSE 0 END) AS Present,\n",
    "    SUM(CASE WHEN status = 'Absent' THEN 1 ELSE 0 END) AS Absent,\n",
    "    SUM(CASE WHEN status = 'Leave' THEN 1 ELSE 0 END) AS Leave\n",
    "  FROM attendance\n",
    "  GROUP BY employee_id\n",
    "\"\"\")\n",
    "'''\n",
    "\n",
    "'''\n",
    "ðŸ§  Alternative: SQL PIVOT (Spark-only)\n",
    "df_result = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM (\n",
    "    SELECT employee_id, status\n",
    "    FROM attendance\n",
    ")\n",
    "PIVOT (\n",
    "    COUNT(status)\n",
    "    FOR status IN ('Present', 'Absent', 'Leave')\n",
    ")\n",
    "ORDER BY employee_id\n",
    "\"\"\")\n",
    "'''\n",
    "\n",
    "# Display result.\n",
    "df_result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
