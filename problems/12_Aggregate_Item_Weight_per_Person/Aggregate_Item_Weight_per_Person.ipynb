{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf0a23d-2713-434c-b3fb-38fbf375ea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+\n",
      "|   name|  item|total_weight|\n",
      "+-------+------+------------+\n",
      "|  alice|carrot|           4|\n",
      "|  alice|tomato|           6|\n",
      "|  alice| apple|           5|\n",
      "|    bob|banana|           4|\n",
      "|    bob|carrot|           6|\n",
      "|    bob| apple|           1|\n",
      "|charlie|banana|           7|\n",
      "|charlie| apple|           5|\n",
      "|charlie|carrot|           3|\n",
      "|  david|tomato|           8|\n",
      "|  david| apple|           2|\n",
      "|  david|carrot|           3|\n",
      "|  david|banana|           4|\n",
      "+-------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "You are given a dataset that contains records of individuals, the items they own, and the weight of each item. Your task is to compute the total_weight per item for each person.\n",
    "\n",
    "Input Schema\n",
    "Column Name\tData Type\n",
    "name\tString\n",
    "item\tString\n",
    "weight\tInteger\n",
    "Example Input Table\n",
    "name\titem\tweight\n",
    "alice\tcarrot\t1\n",
    "bob\tbanana\t3\n",
    "alice\tcarrot\t4\n",
    "bob\tbanana\t4\n",
    "charlie\tbanana\t3\n",
    "Output Schema\n",
    "Column Name\tData Type\n",
    "name\tString\n",
    "item\tString\n",
    "total_weight\tInteger\n",
    "Example Output Table\n",
    "name\titem\ttotal_weight\n",
    "alice\tcarrot\t5\n",
    "bob\tbanana\t7\n",
    "charlie\tbanana\t3\n",
    "Starter Code\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"alice\", \"carrot\", 1),\n",
    "    (\"alice\", \"carrot\", 3),\n",
    "    (\"alice\", \"tomato\", 4),\n",
    "    (\"alice\", \"tomato\", 2),\n",
    "    (\"alice\", \"apple\", 5),\n",
    "\n",
    "    (\"bob\", \"banana\", 3),\n",
    "    (\"bob\", \"banana\", 1),\n",
    "    (\"bob\", \"carrot\", 2),\n",
    "    (\"bob\", \"carrot\", 4),\n",
    "    (\"bob\", \"apple\", 1),\n",
    "\n",
    "    (\"charlie\", \"banana\", 5),\n",
    "    (\"charlie\", \"banana\", 2),\n",
    "    (\"charlie\", \"carrot\", 3),\n",
    "    (\"charlie\", \"apple\", 4),\n",
    "    (\"charlie\", \"apple\", 1),\n",
    "\n",
    "    (\"david\", \"tomato\", 6),\n",
    "    (\"david\", \"tomato\", 2),\n",
    "    (\"david\", \"carrot\", 3),\n",
    "    (\"david\", \"apple\", 2),\n",
    "    (\"david\", \"banana\", 4)\n",
    "]\n",
    "\n",
    "\n",
    "columns = [\"name\", \"item\", \"weight\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Your logic goes here to create df_result\n",
    "\n",
    "display(df_result)\n",
    "'''\n",
    "# Initialize Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "spark = SparkSession.builder.appName('Spark Playground').getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"alice\", \"carrot\", 1),\n",
    "    (\"alice\", \"carrot\", 3),\n",
    "    (\"alice\", \"tomato\", 4),\n",
    "    (\"alice\", \"tomato\", 2),\n",
    "    (\"alice\", \"apple\", 5),\n",
    "\n",
    "    (\"bob\", \"banana\", 3),\n",
    "    (\"bob\", \"banana\", 1),\n",
    "    (\"bob\", \"carrot\", 2),\n",
    "    (\"bob\", \"carrot\", 4),\n",
    "    (\"bob\", \"apple\", 1),\n",
    "\n",
    "    (\"charlie\", \"banana\", 5),\n",
    "    (\"charlie\", \"banana\", 2),\n",
    "    (\"charlie\", \"carrot\", 3),\n",
    "    (\"charlie\", \"apple\", 4),\n",
    "    (\"charlie\", \"apple\", 1),\n",
    "\n",
    "    (\"david\", \"tomato\", 6),\n",
    "    (\"david\", \"tomato\", 2),\n",
    "    (\"david\", \"carrot\", 3),\n",
    "    (\"david\", \"apple\", 2),\n",
    "    (\"david\", \"banana\", 4)\n",
    "]\n",
    "\n",
    "\n",
    "columns = [\"name\", \"item\", \"weight\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df_result = (\n",
    "  df.groupBy(\"name\", \"item\")\n",
    "  .agg(F.sum(\"weight\").alias(\"total_weight\"))\n",
    ")\n",
    "\n",
    "\n",
    "# Display the final DataFrame\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8ef582-7940-48d2-83ba-b479a114d805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
