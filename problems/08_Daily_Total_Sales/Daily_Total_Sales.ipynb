{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b9d01a0-b886-484a-a26c-f045ff4bae95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 8) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+\n",
      "|store_id| sale_date|daily_total_sales|\n",
      "+--------+----------+-----------------+\n",
      "|       1|2025-05-10|             40.0|\n",
      "|       1|2025-05-11|             30.0|\n",
      "|       2|2025-05-10|             40.0|\n",
      "+--------+----------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "'''\n",
    "You have a big dataset with daily sales information from different Walmart stores. Each row in the dataset shows details like store ID, product ID, sale date, quantity sold, and total sales amount. You need to write a PySpark program that calculates the following:\n",
    "\n",
    "Total sales for each store on a daily basis.\n",
    "\n",
    "Input Schema & Example\n",
    "Column Name\tData Type\n",
    "store_id\tInteger\n",
    "product_id\tInteger\n",
    "sale_date\tString\n",
    "quantity_sold\tInteger\n",
    "total_sales\tDouble\n",
    "Example Input Table\n",
    "store_id\tproduct_id\tsale_date\tquantity_sold\ttotal_sales\n",
    "1\t101\t2025-05-10\t2\t25.00\n",
    "1\t102\t2025-05-10\t1\t15.00\n",
    "1\t103\t2025-05-11\t3\t30.00\n",
    "2\t101\t2025-05-10\t2\t40.00\n",
    "Output Schema, Example & Explanation\n",
    "Column Name\tData Type\n",
    "store_id\tInteger\n",
    "sale_date\tString\n",
    "daily_total_sales\tDouble\n",
    "Example Output Table\n",
    "store_id\tsale_date\tdaily_total_sales\n",
    "1\t2025-05-10\t40.00\n",
    "1\t2025-05-11\t30.00\n",
    "2\t2025-05-10\t40.00\n",
    "Explanation\n",
    "On 2025-05-10, store 1 made $25.00 + $15.00 = $40.00.\n",
    "On 2025-05-11, store 1 made $30.00.\n",
    "Store 2 on 2025-05-10 made $40.00.\n",
    "Starter Code\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (1, 101, \"2025-05-10\", 2, 25.00),\n",
    "    (1, 102, \"2025-05-10\", 1, 15.00),\n",
    "    (1, 103, \"2025-05-11\", 3, 30.00),\n",
    "    (2, 101, \"2025-05-10\", 2, 40.00)\n",
    "]\n",
    "\n",
    "columns = [\"store_id\", \"product_id\", \"sale_date\", \"quantity_sold\", \"total_sales\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "Use display(df) to show the final DataFrame.\n",
    "'''\n",
    "\n",
    "# Initialize Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "spark = SparkSession.builder.appName('Spark Playground').getOrCreate()\n",
    "\n",
    "#Copy the starter code or load the file path available in the problem statement \n",
    "data = [\n",
    "    (1, 101, \"2025-05-10\", 2, 25.00),\n",
    "    (1, 102, \"2025-05-10\", 1, 15.00),\n",
    "    (1, 103, \"2025-05-11\", 3, 30.00),\n",
    "    (2, 101, \"2025-05-10\", 2, 40.00)\n",
    "]\n",
    "\n",
    "columns = [\"store_id\", \"product_id\", \"sale_date\", \"quantity_sold\", \"total_sales\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df_result = (\n",
    "  df.groupBy(\"store_id\", \"sale_date\")\n",
    "  .agg(F.sum(\"total_sales\").alias(\"daily_total_sales\"))\n",
    ")\n",
    "\n",
    "# Display the final DataFrame\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9fa5897-981b-4428-9b9a-02666995e96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+\n",
      "|store_id| sale_date|daily_total_sales|\n",
      "+--------+----------+-----------------+\n",
      "|       1|2025-05-10|             40.0|\n",
      "|       1|2025-05-11|             30.0|\n",
      "|       2|2025-05-10|             40.0|\n",
      "+--------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Bonus Challenge: Can you solve this using Spark SQL and temporary views?\n",
    "'''\n",
    "# Register temporary view\n",
    "df.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "df_result_2 = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        store_id,\n",
    "        sale_date,\n",
    "        SUM(total_sales) AS daily_total_sales\n",
    "    FROM sales\n",
    "    GROUP BY store_id, sale_date\n",
    "    ORDER BY store_id, sale_date\n",
    "\"\"\")\n",
    "\n",
    "df_result_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17b0604-b32c-420f-b7f4-23523429ec1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
