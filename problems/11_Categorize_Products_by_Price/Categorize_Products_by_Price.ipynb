{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8750beaf-a5ff-4339-b47f-72b5008df232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------------+\n",
      "|  product|price|price_category|\n",
      "+---------+-----+--------------+\n",
      "|   Laptop|  800|          High|\n",
      "|    Mouse|   25|           Low|\n",
      "| Keyboard|  150|        Medium|\n",
      "|  Monitor|  300|        Medium|\n",
      "|USB Cable|   50|           Low|\n",
      "|  Printer|  450|        Medium|\n",
      "|Gaming PC| 1200|          High|\n",
      "+---------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "You are given a dataset containing product names and their prices. Your task is to add a new column price_category to classify products into:\n",
    "\n",
    "Low if Products that cost less than 100\n",
    "\n",
    "Medium if Products that cost 100 or more, but less than 500\n",
    "\n",
    "High if Products that cost 500 or more\n",
    "\n",
    "Input Schema\n",
    "Column Name\tData Type\n",
    "product\tString\n",
    "price\tInteger\n",
    "Example Input Table\n",
    "product\tprice\n",
    "Mouse\t25\n",
    "Laptop\t800\n",
    "Output Schema\n",
    "Column Name\tData Type\n",
    "product\tString\n",
    "price\tInteger\n",
    "price_category\tString\n",
    "Example Output Table\n",
    "product\tprice\tprice_category\n",
    "Mouse\t25\tLow\n",
    "Laptop\t800\tHigh\n",
    "Starter Code\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"Laptop\", 800),\n",
    "    (\"Mouse\", 25),\n",
    "    (\"Keyboard\", 150),\n",
    "    (\"Monitor\", 300),\n",
    "    (\"USB Cable\", 50),\n",
    "    (\"Printer\", 450),\n",
    "    (\"Gaming PC\", 1200)\n",
    "]\n",
    "\n",
    "columns = [\"product\", \"price\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Your logic goes here to create df_result\n",
    "\n",
    "display(df_result)\n",
    "'''\n",
    "\n",
    "# Initialize Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "spark = SparkSession.builder.appName('Spark Playground').getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"Laptop\", 800),\n",
    "    (\"Mouse\", 25),\n",
    "    (\"Keyboard\", 150),\n",
    "    (\"Monitor\", 300),\n",
    "    (\"USB Cable\", 50),\n",
    "    (\"Printer\", 450),\n",
    "    (\"Gaming PC\", 1200)\n",
    "]\n",
    "\n",
    "columns = [\"product\", \"price\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df_result = (\n",
    "  df.withColumn(\"price_category\", \n",
    "               F.when(F.col(\"price\") < 100, \"Low\")\n",
    "                .when((F.col(\"price\") >= 100) & (F.col(\"price\") < 500), \"Medium\")\n",
    "                .otherwise(\"High\"))\n",
    ")\n",
    "\n",
    "# Display the final DataFrame using the display() function.\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca82b46-08e0-4ef2-b539-2e11b16a37d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
