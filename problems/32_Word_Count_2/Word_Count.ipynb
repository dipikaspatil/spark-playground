{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b52ff996-eb62-4e86-9cc0-c6225bddac82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       word|count|\n",
      "+-----------+-----+\n",
      "|    amazing|    2|\n",
      "|        and|    1|\n",
      "|      break|    1|\n",
      "| characters|    1|\n",
      "|      click|    1|\n",
      "|       code|    1|\n",
      "|      count|    1|\n",
      "|      daily|    1|\n",
      "|       data|    3|\n",
      "|engineering|    3|\n",
      "|      every|    3|\n",
      "|        fun|    2|\n",
      "|    genuine|    1|\n",
      "|   handling|    1|\n",
      "|      hands|    1|\n",
      "|      hello|    1|\n",
      "|  instantly|    1|\n",
      "|         is|    1|\n",
      "|      joins|    1|\n",
      "|   learning|    2|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "You are working with a text file that contains sentences including punctuation and special characters. Your task is to implement a PySpark program that counts how many times each word appears in the file.\n",
    "\n",
    "Ignore/Remove any non-alphanumeric characters such as . , ! ? ; : - _ @ # % etc.\n",
    "\n",
    "The final output must be sorted by word in alphabetical order.\n",
    "\n",
    "Input Schema & Example\n",
    "Input File\n",
    "The input will be with the following schema:\n",
    "\n",
    "Column Name\tData Type\n",
    "text\tString\n",
    "Example Input\n",
    "Hello world! This is a test.\n",
    "PySpark is-awesome; really awesome?\n",
    "Words... should be countedâ€”even if separated!\n",
    "Repeat words, repeat-words; REPEAT words.\n",
    "Output Schema, Example & Explanation\n",
    "Output Table (df_result)\n",
    "Column Name\tData Type\n",
    "word\tString\n",
    "count\tInteger\n",
    "Example Output Table\n",
    "Here is your table sorted alphabetically by word:\n",
    "\n",
    "word\tcount\n",
    "a\t1\n",
    "awesome\t2\n",
    "be\t1\n",
    "counted\t1\n",
    "is\t1\n",
    "repeat\t3\n",
    "should\t1\n",
    "test\t1\n",
    "this\t1\n",
    "words\t4\n",
    "ðŸ’¡ Explanation\n",
    "The data is cleansed.\n",
    "Words are grouped and counted.\n",
    "The DataFrame is sorted by word in alphabetical order.\n",
    "Starter Code\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Step 1: Initialize Spark session\n",
    "spark = SparkSession.builder.appName('Spark Playground').getOrCreate()\n",
    "\n",
    "text = \"\"\"\n",
    "Hello world! Welcome to PySpark.\n",
    "This platform-is amazing; truly amazing.\n",
    "Data-engineering, data_engineering... DATA engineering?\n",
    "Count every-word: every.word; every_word!\n",
    "Special characters: @#$%,.^&*() should not break the logic.\n",
    "Repeat repeat-words; REPEAT_words... repeat!\n",
    "Spark Playground makes learning PySpark fun-really fun.\n",
    "People test joins, windows, and null-handling daily.\n",
    "Hands-on practice, real-world problems... genuine learning!\n",
    "Type code, click run, see results-instantly.\n",
    "\"\"\"\n",
    "\n",
    "# Step 2: Convert multiline string to a DataFrame\n",
    "df = spark.createDataFrame([(text,)], [\"text\"])\n",
    "\n",
    "# Your transformation code here\n",
    "\n",
    "# Final DataFrame should be stored in this variable\n",
    "df_result = ...\n",
    "\n",
    "# Show output\n",
    "display(df_result)\n",
    "\n",
    "'''\n",
    "# Initialize Spark session\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "spark = SparkSession.builder.appName('Spark Playground').getOrCreate()\n",
    "\n",
    "text = \"\"\"\n",
    "Hello world! Welcome to PySpark.\n",
    "This platform-is amazing; truly amazing.\n",
    "Data-engineering, data_engineering... DATA engineering?\n",
    "Count every-word: every.word; every_word!\n",
    "Special characters: @#$%,.^&*() should not break the logic.\n",
    "Repeat repeat-words; REPEAT_words... repeat!\n",
    "Spark Playground makes learning PySpark fun-really fun.\n",
    "People test joins, windows, and null-handling daily.\n",
    "Hands-on practice, real-world problems... genuine learning!\n",
    "Type code, click run, see results-instantly.\n",
    "\"\"\"\n",
    "\n",
    "# Step 2: Convert multiline string to a DataFrame\n",
    "df = spark.createDataFrame([(text,)], [\"text\"])\n",
    "\n",
    "df_result = (\n",
    "  # keep only letters and spaces â€” replace everything else with space\n",
    "  # (r here is \"raw string\" in case if we add \\s in future if we need to handle whitespace)\n",
    "  # good practice is to add r\n",
    "  df.withColumn(\"clean\", F.regexp_replace(F.col(\"text\"), r\"[^A-Za-z0-9]+\", \" \"))\n",
    "  .withColumn(\"word\", F.explode(F.split(F.col(\"clean\"), r\"\\s+\"))) # split into words, r - raw string\n",
    "  .filter(F.col(\"word\") != \"\") # remove blanks\n",
    "  .withColumn(\"word\", F.lower(F.col(\"word\"))) # lowercase words\n",
    "  .groupBy(\"word\")\n",
    "  .agg(F.count(\"word\").alias(\"count\"))\n",
    "  .orderBy(\"word\") #sort alphabetically by word\n",
    ")\n",
    "\n",
    "# Display result.\n",
    "df_result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
